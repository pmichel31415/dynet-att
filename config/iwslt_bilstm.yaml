# General arguments
exp_name: iwslt_bilstm
verbose: True
output_dir: "output"
src_lang: fr
tgt_lang: en
# Tokenization
tokenizer_type: subword
subword_algo: unigram
subword_train_files:
  - datasets/iwslt/iwslt2016.fr-en/train.fr-en.fr
  - datasets/iwslt/iwslt2016.fr-en/train.fr-en.en
subword_voc_size: 16000
# Model
architecture: medium_bilstm
# Training specific arguments
train:
  task: train
  # Tokenizer args
  train_src: datasets/iwslt/iwslt2016.fr-en/train.fr-en.fr
  train_tgt: datasets/iwslt/iwslt2016.fr-en/train.fr-en.en
  valid_src: datasets/iwslt/iwslt2016.fr-en/tst2013.fr-en.fr
  valid_tgt: datasets/iwslt/iwslt2016.fr-en/tst2013.fr-en.en
  # Optimization args
  n_epochs: 10
  objective: nll
  label_smoothing: 0.1
  trainer: adam
  learning_rate: 0.001
  learning_rate_decay: 0.5
  gradient_clip: 5.0
  batch_size: 64
  max_tokens_per_batch: 2000
  valid_batch_size: 10
eval_ppl:
  task: eval_ppl
  eval_src: datasets/iwslt/iwslt2016.fr-en/tst2013.fr-en.fr
  eval_tgt: datasets/iwslt/iwslt2016.fr-en/tst2013.fr-en.en
  eval_batch_size: 32
translate:
  task: translate
  trans_src: datasets/iwslt/iwslt2016.fr-en/tst2014.fr-en.fr
  trans_batch_size: 1
  out_file: output/iwslt_bilstm.tst2014.fr-en.out.en
  max_len: 150
  beam_size: 4
  lenpen: 1.0
